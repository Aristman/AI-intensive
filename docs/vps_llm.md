# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é TinyLlama-1.1B –Ω–∞ VPS

### 1. **–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞**
```bash
# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
sudo apt update && sudo apt upgrade -y

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤
sudo apt install -y python3-pip python3-venv git wget build-essential
```

### 2. **–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è**
```bash
# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞
mkdir tinylama-app && cd tinylama-app

# –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è
python3 -m venv venv
source venv/bin/activate
```

### 3. **–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π**
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch –¥–ª—è CPU (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
pip install transformers accelerate sentencepiece uvicorn fastapi
```

### 4. **–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ TinyLlama-1.1B**
–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `download_model.py`:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto"
)
print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
```

–ó–∞–ø—É—Å—Ç–∏—Ç–µ –∑–∞–≥—Ä—É–∑–∫—É:
```bash
python download_model.py
```

### 5. **–°–æ–∑–¥–∞–Ω–∏–µ API-—Å–µ—Ä–≤–µ—Ä–∞**
–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `app.py`:
```python
from fastapi import FastAPI
from transformers import AutoModelForCausalLM, AutoTokenizer
import uvicorn
import torch

app = FastAPI()

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto"
)

@app.post("/generate")
async def generate_text(prompt: str, max_length: int = 100):
    # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è TinyLlama
    formatted_prompt = f"<|system|>\nYou are a helpful assistant.</s>\n<|user|>\n{prompt}</s>\n<|assistant|>\n"
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    inputs = tokenizer(formatted_prompt, return_tensors="pt")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    assistant_response = response.split("<|assistant|>")[-1].strip()
    
    return {"response": assistant_response}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 6. **–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞**
```bash
# –ó–∞–ø—É—Å–∫ –Ω–∞–ø—Ä—è–º—É—é
python app.py

# –ò–ª–∏ –∑–∞–ø—É—Å–∫ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ —Å nohup
nohup python app.py > server.log 2>&1 &
```

### 7. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API**
```bash
curl -X POST "http://localhost:8000/generate" -H "Content-Type: application/json" -d '{"prompt": "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?", "max_length": 150}'

Invoke-WebRequest -Uri "http://158.160.107.227:8000/generate" -Method POST -ContentType "application/json" -Body '{"prompt": "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?", "max_length": 150}'
```

### 8. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –º–∞–ª–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤**
–°–æ–∑–¥–∞–π—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é `app_optimized.py`:
```python
from fastapi import FastAPI
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import uvicorn

app = FastAPI()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ pipeline –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
pipe = pipeline(
    "text-generation",
    model=model_name,
    device="cpu",
    torch_dtype="auto"
)

@app.post("/generate")
async def generate_text(prompt: str, max_length: int = 100):
    formatted_prompt = f"<|system|>\nYou are a helpful assistant.</s>\n<|user|>\n{prompt}</s>\n<|assistant|>\n"
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é pipeline
    result = pipe(
        formatted_prompt,
        max_length=max_length,
        temperature=0.7,
        do_sample=True,
        pad_token_id=pipe.tokenizer.eos_token_id
    )
    
    response = result[0]['generated_text']
    assistant_response = response.split("<|assistant|>")[-1].strip()
    
    return {"response": assistant_response}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, workers=1)
```

### 9. **–°–æ–∑–¥–∞–Ω–∏–µ —Å–ª—É–∂–±—ã systemd –¥–ª—è –∞–≤—Ç–æ–∑–∞–ø—É—Å–∫–∞**
–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `/etc/systemd/system/tinylama.service`:
```ini
[Unit]
Description=TinyLlama API Service
After=network.target

[Service]
User=root
WorkingDirectory=/root/tinylama-app
Environment="PATH=/root/tinylama-app/venv/bin"
ExecStart=/root/tinylama-app/venv/bin/python /root/tinylama-app/app_optimized.py
Restart=always

[Install]
WantedBy=multi-user.target
```

–ê–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ —Å–ª—É–∂–±—É:
```bash
sudo systemctl daemon-reload
sudo systemctl start tinylama
sudo systemctl enable tinylama
```

### 10. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ**
```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å–ª—É–∂–±—ã
sudo systemctl status tinylama

# –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤
journalctl -u tinylama -f

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–ª—É–∂–±—ã
sudo systemctl stop tinylama
```

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:

1. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏**:
    - –ú–æ–¥–µ–ª—å –∑–∞–Ω–∏–º–∞–µ—Ç –æ–∫–æ–ª–æ 2.2 –ì–ë –≤ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏
    - –î–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ:
   ```python
   model = AutoModelForCausalLM.from_pretrained(
       model_name,
       device_map="auto",
       load_in_4bit=True,  # 4-–±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ
       bnb_4bit_compute_dtype=torch.float16
   )
   ```

2. **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å**:
   ```bash
   # –û—Ç–∫—Ä—ã—Ç–∏–µ –ø–æ—Ä—Ç–∞ –≤ —Ñ–∞–µ—Ä–≤–æ–ª–µ
   sudo ufw allow 8000
   sudo ufw enable
   ```

3. **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**:
    - –î–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ —É–º–µ–Ω—å—à–∏—Ç–µ `max_length`
    - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
    - –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª–µ–µ –ª–µ–≥–∫–æ–≥–æ –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, `waitress`)

4. **–†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ**:
    - –ú–æ–¥–µ–ª—å –∑–∞–Ω–∏–º–∞–µ—Ç –æ–∫–æ–ª–æ 2.2 –ì–ë –Ω–∞ –¥–∏—Å–∫–µ
    - –†–µ–≥—É–ª—è—Ä–Ω–æ –¥–µ–ª–∞–π—Ç–µ –±—ç–∫–∞–ø—ã –≤–∞–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–≠—Ç–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å TinyLlama-1.1B –Ω–∞ VPS —Å 8 –ì–ë RAM, –æ—Å—Ç–∞–≤–ª—è—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –∏ –¥—Ä—É–≥–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤.


# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ API –¥–ª—è TinyLlama-1.1B

–°–æ–∑–¥–∞–¥–∏–º API, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å —Ñ–æ—Ä–º–∞—Ç–æ–º –∑–∞–ø—Ä–æ—Å–æ–≤ OpenAI, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏.

## 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install pydantic
```

## 2. –°–æ–∑–¥–∞–Ω–∏–µ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ API

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `openai_app.py`:

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
import uvicorn
import torch
from threading import Thread
from typing import List, Optional, Dict, Any
import time
import shortuuid
import json

app = FastAPI(title="TinyLlama OpenAI-Compatible API")

# –ú–æ–¥–µ–ª–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Å OpenAI API
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str = "TinyLlama-1.1B-Chat"
    messages: List[ChatMessage]
    temperature: float = 0.7
    top_p: float = 0.9
    max_tokens: int = 512
    stream: bool = False

class UsageInfo(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponseChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: str

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionResponseChoice]
    usage: UsageInfo

class ModelCard(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str = "local"

class ModelList(BaseModel):
    object: str = "list"
    data: List[ModelCard]

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
print("Loading model...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto",
    low_cpu_mem_usage=True
)
print("Model loaded successfully!")

# –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è TinyLlama
def format_prompt(messages: List[ChatMessage]) -> str:
    formatted_prompt = ""
    
    for message in messages:
        if message.role == "system":
            formatted_prompt += f"<|system|>\n{message.content}</s>\n"
        elif message.role == "user":
            formatted_prompt += f"<|user|>\n{message.content}</s>\n"
        elif message.role == "assistant":
            formatted_prompt += f"<|assistant|>\n{message.content}</s>\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥ –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    formatted_prompt += "<|assistant|>\n"
    
    return formatted_prompt

# –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π
@app.get("/v1/models", response_model=ModelList)
async def list_models():
    model_card = ModelCard(
        id="TinyLlama-1.1B-Chat",
        created=int(time.time()),
        owned_by="local"
    )
    return ModelList(data=[model_card])

# –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —á–∞—Ç-–∑–∞–ø—Ä–æ—Å–æ–≤
@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def create_chat_completion(request: ChatCompletionRequest):
    try:
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        prompt = format_prompt(request.messages)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        inputs = tokenizer(prompt, return_tensors="pt")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        assistant_response = response_text.split("<|assistant|>")[-1].strip()
        
        # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
        prompt_tokens = len(tokenizer.encode(prompt))
        completion_tokens = len(tokenizer.encode(assistant_response))
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ç–≤–µ—Ç
        response = ChatCompletionResponse(
            id=f"chatcmpl-{shortuuid.random()}",
            created=int(time.time()),
            model=request.model,
            choices=[
                ChatCompletionResponseChoice(
                    index=0,
                    message=ChatMessage(
                        role="assistant",
                        content=assistant_response
                    ),
                    finish_reason="stop"
                )
            ],
            usage=UsageInfo(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens
            )
        )
        
        return response
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è
@app.get("/health")
async def health_check():
    return {"status": "healthy"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## 3. –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª—É–∂–±—ã systemd

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `/etc/systemd/system/tinylama-openai.service`:

```ini
[Unit]
Description=TinyLlama OpenAI API Service
After=network.target

[Service]
User=root
WorkingDirectory=/root/tinylama-app
Environment="PATH=/root/tinylama-app/venv/bin"
ExecStart=/root/tinylama-app/venv/bin/python /root/tinylama-app/openai_app.py
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

–ê–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ —Å–ª—É–∂–±—É:

```bash
sudo systemctl daemon-reload
sudo systemctl start tinylama-openai
sudo systemctl enable tinylama-openai
```

## 4. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API

### –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π:

```bash
curl http://localhost:8000/v1/models
```

### –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI:

```bash
curl -X POST "http://localhost:8000/v1/chat/completions" -H "Content-Type: application/json" -d '{
  "model": "TinyLlama-1.1B-Chat",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?"}
  ],
  "temperature": 0.7,
  "max_tokens": 150
}'
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å –∫–ª–∏–µ–Ω—Ç—Å–∫–∏–º–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏ OpenAI:

–ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ Python, –≤—ã –º–æ–∂–µ—Ç–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∫–ª–∏–µ–Ω—Ç OpenAI –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∞—à–∏–º API:

```python
import openai

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∞—à–µ–≥–æ API
client = openai.OpenAI(
    base_url="http://your-server-ip:8000/v1",
    api_key="sk-no-key-required"  # –ú–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –ª—é–±–æ–π –∫–ª—é—á
)

# –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
response = client.chat.completions.create(
    model="TinyLlama-1.1B-Chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?"}
    ],
    temperature=0.7,
    max_tokens=150
)

print(response.choices[0].message.content)
```

## 5. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ CORS middleware:

```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏:

```python
from fastapi import Depends, Header
from typing import Annotated

async def verify_token(authorization: Annotated[str, Header()] = None):
    if authorization is None or not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")
    
    token = authorization[7:]
    if token != "your-secret-token":
        raise HTTPException(status_code=401, detail="Invalid token")
    
    return token

# –î–æ–±–∞–≤—å—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –∫ —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞–º:
@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def create_chat_completion(
    request: ChatCompletionRequest, 
    token: str = Depends(verify_token)
):
    # –≤–∞—à –∫–æ–¥
```

## 6. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

–î–æ–±–∞–≤—å—Ç–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤:

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("api.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# –í —ç–Ω–¥–ø–æ–∏–Ω—Ç–µ –¥–æ–±–∞–≤—å—Ç–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ:
logger.info(f"Received request: {request}")
```

–≠—Ç–æ—Ç API —Ç–µ–ø–µ—Ä—å —Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Ñ–æ—Ä–º–∞—Ç–æ–º –∑–∞–ø—Ä–æ—Å–æ–≤ OpenAI, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏–º–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏.

–û—Ç–ª–∏—á–Ω–æ! –Ø —Å–æ–∑–¥–∞–ª –ø–æ–ª–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è –≤–∞—à–µ–≥–æ TinyLlama OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ API. –í–æ—Ç —á—Ç–æ –±—ã–ª–æ —Å–æ–∑–¥–∞–Ω–æ:

üìÑ –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã:

1. README.md - –ü–æ–¥—Ä–æ–±–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å:
   ‚Ä¢  –û–±–∑–æ—Ä–æ–º –ø—Ä–æ–µ–∫—Ç–∞
   ‚Ä¢  –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ
   ‚Ä¢  –û–ø–∏—Å–∞–Ω–∏–µ–º –≤—Å–µ—Ö API —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤
   ‚Ä¢  –ü—Ä–∏–º–µ—Ä–∞–º–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤
   ‚Ä¢  –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–º –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é –Ω–µ–ø–æ–ª–∞–¥–æ–∫
2. requirements.txt - –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π Python
3. test_api.py - –¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ API
4. Dockerfile - –î–ª—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
5. docker-compose.yml - –î–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ —Å Docker Compose

üöÄ –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ API:

–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç:
‚Ä¢  POST /v1/chat/completions - OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API –¥–ª—è —á–∞—Ç-–¥–∏–∞–ª–æ–≥–æ–≤

–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞:
‚Ä¢  model: "TinyLlama-1.1B-Chat" (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
‚Ä¢  messages: –º–∞—Å—Å–∏–≤ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ä–æ–ª—è–º–∏ system/user/assistant
‚Ä¢  temperature: 0.7 (–∫–æ–Ω—Ç—Ä–æ–ª—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏)
‚Ä¢  max_tokens: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–ª–∏ –∑–∞–¥–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
‚Ä¢  top_p: 0.9 (nucleus sampling)
‚Ä¢  stream: false (–ø–æ—Ç–æ–∫–æ–≤–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç:
‚Ä¢  POST /v1/token_count - –ø–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ

–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:
‚Ä¢  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: 2048 —Ç–æ–∫–µ–Ω–æ–≤
‚Ä¢  –ú–æ–¥–µ–ª—å: TinyLlama/TinyLlama-1.1B-Chat-v1.0 (1.1B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
‚Ä¢  –§–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–æ–≤: ChatML-–ø–æ–¥–æ–±–Ω—ã–π

üìù –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç:
‚Ä¢  cURL –∫–æ–º–∞–Ω–¥—ã
‚Ä¢  Python —Å requests
‚Ä¢  JavaScript/Node.js
‚Ä¢  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º OpenAI Python SDK

–¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å –ø–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∞—à–µ–≥–æ TinyLlama API —Å–µ—Ä–≤–µ—Ä–∞!